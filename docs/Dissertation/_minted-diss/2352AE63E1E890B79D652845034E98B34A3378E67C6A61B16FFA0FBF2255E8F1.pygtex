\begin{Verbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{forward\PYGZus{}propagation}\PYG{p}{(}\PYG{n}{inputs}\PYG{p}{):}

    \PYG{c+c1}{\PYGZsh{} Initialise the cache used to store activations and weighted sums}
    \PYG{n}{activations} \PYG{o}{:=} \PYG{p}{[]}
    \PYG{n}{activations}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]} \PYG{o}{:=} \PYG{n}{inputs}
    \PYG{n}{final} \PYG{o}{:=} \PYG{n}{number} \PYG{n}{of} \PYG{n}{layers} \PYG{o+ow}{in} \PYG{n}{the} \PYG{n}{neural} \PYG{n}{network}

    \PYG{c+c1}{\PYGZsh{} Calculate weighted sum (Z) and activation (A) at each layer}
    \PYG{k}{for} \PYG{n}{layer} \PYG{o+ow}{in} \PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{final\PYGZus{}layer}\PYG{p}{):}
    	\PYG{n}{Z} \PYG{o}{:=} \PYG{n}{dot}\PYG{p}{(}\PYG{n}{weights}\PYG{p}{[}\PYG{n}{layer}\PYG{p}{],} \PYG{n}{activations}\PYG{p}{[}\PYG{n}{layer}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{])} \PYG{o}{+} \PYG{n}{biases}\PYG{p}{[}\PYG{n}{layer}\PYG{p}{]}
    	\PYG{n}{activations} \PYG{o}{:=} \PYG{n}{activation\PYGZus{}function}\PYG{p}{(}\PYG{n}{Z}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} Calculate activation on the final layer}
    \PYG{n}{Z} \PYG{o}{:=} \PYG{n}{dot}\PYG{p}{(}\PYG{n}{weights}\PYG{p}{[}\PYG{n}{final}\PYG{p}{],} \PYG{n}{activations}\PYG{p}{[}\PYG{n}{final}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{])} \PYG{o}{+} \PYG{n}{biases}\PYG{p}{[}\PYG{n}{final}\PYG{p}{]}
    \PYG{n}{activations}\PYG{p}{[}\PYG{n}{final}\PYG{p}{]} \PYG{o}{:=} \PYG{n}{sigmoid}\PYG{p}{(}\PYG{n}{Z}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} Round final layer to 0 or 1 and return}
    \PYG{n}{outputs} \PYG{o}{:=} \PYG{n+nb}{round}\PYG{p}{(}\PYG{n}{activations}\PYG{p}{[}\PYG{n}{final}\PYG{p}{],} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{)}
    \PYG{k}{return} \PYG{n}{outputs}
\end{Verbatim}
